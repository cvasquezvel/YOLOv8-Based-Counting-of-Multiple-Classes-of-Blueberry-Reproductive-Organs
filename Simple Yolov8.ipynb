{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be51dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tempfile\n",
    "# import os\n",
    "\n",
    "# # Crear un directorio temporal\n",
    "# temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "# # Realizar operaciones en el directorio temporal\n",
    "\n",
    "# # Eliminar el directorio temporal y su contenido\n",
    "# os.rmdir(temp_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4577631",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Cambiar el directorio de trabajo actual\n",
    "os.chdir(\"G:\\\\Mi unidad\\\\Python\\\\Proyecto-deteccion-bayas\")\n",
    "\n",
    "# Ahora, las rutas relativas se basarán en el nuevo directorio actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc942e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'g:\\\\Mi unidad\\\\Python\\\\Proyecto-deteccion-bayas'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtener la ruta actual del directorio\n",
    "current_directory = os.getcwd()\n",
    "current_directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55649e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3738fad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f720d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7923657",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff3a7f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d1432d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from opencv-python) (1.26.2)\n"
     ]
    }
   ],
   "source": [
    "#!pip install opencv-python==4.7.0.72\n",
    "!pip install opencv-python\n",
    "#!conda remove opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19bf624f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\cv2\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682cdf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import os\n",
    "import cv2 as cv\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from glob import glob\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import random\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import yaml\n",
    "plt.style.use(\"ggplot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8296f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cv.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4b631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --user roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1112392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from roboflow import Roboflow\n",
    "# rf = Roboflow(api_key=\"ysLbfsnaroQianBM7EmF\")\n",
    "# project = rf.workspace(\"inkastats-sac\").project(\"blueberries-detection\")\n",
    "# dataset = project.version(4).download(\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e4574f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = \"C:/Users/cvasquezv/Proyecto-vigor-defoliacion/input\"\n",
    "annotation_list = glob(\"C:/Users/cvasquezv/Proyecto-vigor-defoliacion/input/mortalidad/annotations/*.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d1c7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame()\n",
    "data[\"annotation_dir\"] = annotation_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e27fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f782c82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ruta_completa = \"C:/Users/cvasquezv/Proyecto-vigor-defoliacion/input/4712331.FOTO.121358.jpg\"\n",
    "nombre_archivo = os.path.basename(ruta_completa)\n",
    "\n",
    "print(nombre_archivo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4f553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"C:/Users/cvasquezv/Proyecto-vigor-defoliacion/input../../../input/4712331.FOTO.121358.jpg\"\n",
    "print(\"Antes de las sustituciones:\", img_dir)\n",
    "\n",
    "img_dir = img_dir.replace(\"/input../../../input/\", \"/\").replace(\"/input../../\", \"/\").replace(\"/input../\", \"/input/\")\n",
    "print(\"Después de las sustituciones:\", img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4d649b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "def read_json_data(jsonfile):\n",
    "    rows = {\"img_dir\":[] ,\"img_w\":[] , \"img_h\" : [], \"sp_type\": [] ,\"xc\":[] , \"yc\":[], \"bb_height\" : [] , \"bb_width\": [] , \"bb_left\":[] , \"bb_top\":[]}\n",
    "    \n",
    "    # read file\n",
    "    with open(jsonfile) as json_f:\n",
    "        json_data = json.load(json_f)\n",
    "\n",
    "        img_dir = json_data[\"imagePath\"].replace(\"\\\\\", \"/\")\n",
    "        #img_dir = img_dir.split(\"./input\")[1]  # Corregir la construcción de la ruta\n",
    "\n",
    "        img_dir = img_dir.replace(\"../../../input/\", \"/\").replace(\"../../\", \"/\").replace(\"..\", \"\")\n",
    "        #img_dir = img_dir.replace(\"../..\", \"/\")\n",
    "        \n",
    "        sp_type = json_data[\"shapes\"][0][\"label\"]\n",
    "\n",
    "        img_w = json_data[\"imageWidth\"]\n",
    "        img_h = json_data[\"imageHeight\"]\n",
    "\n",
    "        # Normalize Bounding Box\n",
    "        bb_points = json_data[\"shapes\"][0][\"points\"]\n",
    "        x1, y1 = bb_points[0]\n",
    "        x2, y2 = bb_points[1]\n",
    "\n",
    "        # Obtén las coordenadas del centro de la bounding box\n",
    "        xc = (x1 + x2) / (2 * img_w)\n",
    "        yc = (y1 + y2) / (2 * img_h)\n",
    "\n",
    "        # Obtén el ancho y alto de la bounding box\n",
    "        bb_width = abs(x2 - x1) / img_w\n",
    "        bb_height = abs(y2 - y1) / img_h\n",
    "\n",
    "        # Normalizar las coordenadas izquierda y superior de la bounding box\n",
    "        bb_left = min(x1, x2) / img_w\n",
    "        bb_top = min(y1, y2) / img_h\n",
    "\n",
    "        rows[\"img_dir\"].append(BASE_DIR + img_dir)\n",
    "        rows[\"sp_type\"].append(sp_type)\n",
    "        rows[\"img_w\"].append(img_w)\n",
    "        rows[\"img_h\"].append(img_h)\n",
    "        rows[\"bb_height\"].append(bb_height)\n",
    "        rows[\"bb_width\"].append(bb_width)\n",
    "        rows[\"bb_left\"].append(bb_left)\n",
    "        rows[\"bb_top\"].append(bb_top)\n",
    "        rows[\"xc\"].append(xc)\n",
    "        rows[\"yc\"].append(yc)\n",
    "\n",
    "    return rows\n",
    "\n",
    "data_list = []\n",
    "\n",
    "for json_file in annotation_list:\n",
    "    data_list.append(pd.DataFrame(read_json_data(json_file)))\n",
    "\n",
    "df = pd.concat(data_list, ignore_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faebd192",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.concat(data_list, ignore_index=True)\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e120a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for json_file in tqdm(data[\"annotation_dir\"]):\n",
    "#    rows = read_json_data(json_file)\n",
    "    \n",
    "    # Crea un nuevo DataFrame a partir del diccionario y luego concaténalo con el DataFrame principal\n",
    "#    df = pd.concat([df, pd.DataFrame.from_dict(rows)], ignore_index=True)\n",
    "\n",
    "# Ahora df contiene todos los datos combinados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4d7fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)\n",
    "\n",
    "df['sp_type'] = df['sp_type'].str.strip().replace({\"0\": \"Dead\", \"1\": \"Alive\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55e613e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b164943",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['img_dir'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cf8356",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd57741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "\n",
    "# Guardar el DataFrame en un archivo Excel\n",
    "df.to_excel('data info.xlsx', index=False)  # Cambia 'nombre_del_archivo.xlsx' por el nombre que desees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa2e8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sp_type.value_counts().plot(kind='bar')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ebff56",
   "metadata": {},
   "source": [
    "## Display Some examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c024b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "def display_random_imgs(df, rows, cols):\n",
    "    idxs = random.sample(df.index.tolist(), rows * cols)\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(10, 10))\n",
    "\n",
    "    if rows * cols != 1:\n",
    "        for count, axs in enumerate(ax.flatten()):\n",
    "            path = df.img_dir.iloc[idxs[count]]\n",
    "            img = plt.imread(path)\n",
    "\n",
    "            # Obtén las coordenadas normalizadas de la bounding box\n",
    "            x = df.bb_left.iloc[idxs[count]]\n",
    "            y = df.bb_top.iloc[idxs[count]]\n",
    "            w = df.bb_width.iloc[idxs[count]]\n",
    "            h = df.bb_height.iloc[idxs[count]]\n",
    "\n",
    "            # Convierte las coordenadas normalizadas a píxeles\n",
    "            x_pixel = x * df.img_w.iloc[idxs[count]]\n",
    "            y_pixel = y * df.img_h.iloc[idxs[count]]\n",
    "            w_pixel = w * df.img_w.iloc[idxs[count]]\n",
    "            h_pixel = h * df.img_h.iloc[idxs[count]]\n",
    "\n",
    "            # Crea el rectángulo\n",
    "            patch = patches.Rectangle((x_pixel, y_pixel), w_pixel, h_pixel,\n",
    "                                      linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "            axs.imshow(img)\n",
    "            axs.add_patch(patch)\n",
    "            axs.axis('off')\n",
    "            axs.set_title(df.sp_type.iloc[idxs[count]])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Llamada a la función\n",
    "# display_random_imgs(df, 2, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbe2ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_random_imgs(df ,5, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef62ba32",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes_name = [\"Dead\",\"Alive\"]\n",
    "classes_num = [0,1]\n",
    "\n",
    "df.sp_type = df.sp_type.replace(classes_name , classes_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc9c114",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdf44fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Suponiendo que df es tu DataFrame original\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento, validación y prueba\n",
    "train_data, test_data = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_data, valid_data = train_test_split(train_data, test_size=0.25, random_state=42)  # El 25% de train para validación\n",
    "\n",
    "# Ahora, 'train', 'val' y 'test' contienen los conjuntos de entrenamiento, validación y prueba respectivamente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f2715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Obtener el directorio actual del script\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Crear directorios relativos al directorio actual\n",
    "os.makedirs(os.path.join(current_directory, \"working/data/train\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(current_directory, \"working/data/valid\"), exist_ok=True)\n",
    "os.makedirs(os.path.join(current_directory, \"working/data/test\"), exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c0bcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Asignar datos a los directorios correspondientes\n",
    "def add_data_to_folder(file_type, data):\n",
    "    for index, row in tqdm(data.iterrows(), total=len(data)):\n",
    "        shutil.copy(row[\"img_dir\"], f\"working/data/{file_type}\")\n",
    "        with open(f'working/data/{file_type}/{row.img_dir.split(\"/\")[-1][:-4]}.txt', \"w\") as f:\n",
    "            f.write(str(row[\"sp_type\"]))\n",
    "            f.write(\" \")\n",
    "            f.write(str(row[\"xc\"]))\n",
    "            f.write(\" \")\n",
    "            f.write(str(row[\"yc\"]))\n",
    "            f.write(\" \")\n",
    "            f.write(str(row[\"bb_width\"]))\n",
    "            f.write(\" \")\n",
    "            f.write(str(row[\"bb_height\"]))\n",
    "            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15afc814",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def add_data_to_folder(file_type, data):\n",
    "#    for index, row in tqdm(data.iterrows(), total=len(data)):\n",
    "#        shutil.copy(row[\"img_dir\"], f\"working/data/{file_type}\")\n",
    "#        with open(f'working/data/{file_type}/{row.img_dir.split(\"/\")[-1][:-4]}.txt', \"w\") as f:\n",
    "#            f.write(str(row[\"sp_type\"]))  # escribir la clase\n",
    "#            f.write(\" \")\n",
    "#            f.write(str(row[\"bb_left\"]))\n",
    "#            f.write(\" \")\n",
    "#            f.write(str(row[\"bb_top\"]))\n",
    "#            f.write(\" \")\n",
    "#            f.write(str(row[\"bb_width\"]))\n",
    "#            f.write(\" \")\n",
    "#            f.write(str(row[\"bb_height\"]))\n",
    "#            f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ca74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(123)  # Establece la semilla de aleatorización\n",
    "\n",
    "# A continuación, puedes usar funciones de generación de números aleatorios, como random.randint(), etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d322ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luego puedes llamar a la función para cada conjunto de datos\n",
    "# Asignar a train\n",
    "add_data_to_folder(\"train\", train_data)\n",
    "\n",
    "# Asignar a valid\n",
    "add_data_to_folder(\"valid\", valid_data)\n",
    "\n",
    "# Asignar a test\n",
    "add_data_to_folder(\"test\", test_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create yml file\n",
    "#yaml_dict = dict(\n",
    "#    train = 'C:/Users/cvasquezv/kaggle/working/data/train',\n",
    "#    val = 'C:/Users/cvasquezv/kaggle/working/data/test',\n",
    "#    \n",
    "#    nc    = len(classes_num), # number of classes\n",
    "#    names = classes_name # classes\n",
    "#    )\n",
    "\n",
    "#with open('C:/Users/cvasquezv/kaggle/working/data.yaml', 'w') as outfile:\n",
    "#    yaml.dump(yaml_dict, outfile, default_flow_style=False)\n",
    "\n",
    "#%cat C:/Users/cvasquezv/kaggle/working/data.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b0e436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyyaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade2f014",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "# Definir la información del YAML\n",
    "yaml_data = {\n",
    "    'train': 'C:\\\\Users\\\\cvasquezv\\\\Proyecto-vigor-defoliacion\\\\working\\\\data\\\\train\\\\',\n",
    "    'val': 'C:\\\\Users\\\\cvasquezv\\\\Proyecto-vigor-defoliacion\\\\working\\\\data\\\\valid\\\\',\n",
    "    #'test': 'C:\\\\Users\\\\cvasquezv\\\\Proyecto-vigor-defoliacion\\\\working\\\\data\\\\test\\\\',\n",
    "    'nc': 2,\n",
    "    'names': [\"Dead\",\"Alive\"]#,\n",
    "    # 'num': [\"0\",\"1\"]\n",
    "    \n",
    "}\n",
    "\n",
    "# Guardar el YAML en un archivo en el directorio working\n",
    "yaml_file_path = 'C:\\\\Users\\\\cvasquezv\\\\Proyecto-vigor-defoliacion\\\\working\\\\data.yaml'\n",
    "with open(yaml_file_path, 'w') as yaml_file:\n",
    "    yaml.dump(yaml_data, yaml_file, default_flow_style=False)\n",
    "\n",
    "print(f\"Archivo YAML generado y guardado en: {yaml_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16434c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('C:/Users/cvasquezv/Proyecto-vigor-defoliacion/working/data.yaml', 'r') as infile:\n",
    "    yaml_content = infile.read()\n",
    "\n",
    "print(yaml_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51946cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\cvasquezv\\Proyecto-vigor-defoliacion\\working\\data\\train'\n",
    "\n",
    "# Obtener la lista de archivos de anotación .txt en el directorio\n",
    "annotation_files = [f for f in os.listdir(data_path) if f.endswith('.txt')]\n",
    "\n",
    "# Crear un DataFrame para almacenar la información\n",
    "df_train = pd.DataFrame(columns=['class'])\n",
    "\n",
    "# Analizar cada archivo de anotación\n",
    "for annotation_file in annotation_files:\n",
    "    annotation_file_path = os.path.join(data_path, annotation_file)\n",
    "\n",
    "    with open(annotation_file_path, 'r') as f:\n",
    "        # Cada línea del archivo .txt representa una bounding box\n",
    "        lines = f.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            # La primera columna en cada línea es el número de clase\n",
    "            # Puedes adaptar esto según el formato específico de tus archivos de anotación\n",
    "            class_number = line.split()[0]\n",
    "\n",
    "            # Agregar la clase al DataFrame\n",
    "            df_train = pd.concat([df_train, pd.DataFrame({'class': [class_number]})], ignore_index=True)\n",
    "\n",
    "# Contar la frecuencia de cada clase\n",
    "class_counts = df_train['class'].value_counts()\n",
    "\n",
    "# Crear un gráfico de barras para visualizar la distribución de clases\n",
    "plt.figure(figsize=(10, 6))\n",
    "class_counts.plot(kind='bar', color='skyblue')\n",
    "plt.title('Distribución de Clases en Conjunto de Datos YOLO')\n",
    "plt.xlabel('Número de Clase')\n",
    "plt.ylabel('Número de Bounding Boxes')\n",
    "plt.show()\n",
    "\n",
    "# Imprimir la tabla de frecuencia\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a12208",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5eb8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12fa1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_images_with_rectangles(data_dir, rows, cols):\n",
    "    # Obtén la lista de archivos de imágenes y anotaciones\n",
    "    image_files = [f for f in os.listdir(data_dir) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.gif'))]\n",
    "    annotation_files = [f for f in os.listdir(data_dir) if f.lower().endswith('.txt')]\n",
    "\n",
    "    # Filtra las imágenes que tienen anotaciones asociadas\n",
    "    valid_image_files = [img for img in image_files if f\"{os.path.splitext(img)[0]}.txt\" in annotation_files]\n",
    "\n",
    "    # Selecciona aleatoriamente las imágenes a mostrar\n",
    "    selected_images = random.sample(valid_image_files, rows * cols)\n",
    "\n",
    "    # Crea el subgráfico\n",
    "    fig, ax = plt.subplots(rows, cols, figsize=(10, 10))\n",
    "\n",
    "    if rows * cols != 1:\n",
    "        for count, axs in enumerate(ax.flatten()):\n",
    "            # Ruta de la imagen\n",
    "            img_path = os.path.join(data_dir, selected_images[count])\n",
    "            img = plt.imread(img_path)\n",
    "\n",
    "            # Ruta del archivo de anotación\n",
    "            annotation_path = os.path.join(data_dir, f\"{os.path.splitext(selected_images[count])[0]}.txt\")\n",
    "\n",
    "            # Lee las coordenadas del archivo de anotación\n",
    "            with open(annotation_path, 'r') as annotation_file:\n",
    "                sp_type, xc, yc, bb_width, bb_height = annotation_file.readline().split()\n",
    "                xc, yc, bb_width, bb_height = map(float, [xc, yc, bb_width, bb_height])\n",
    "\n",
    "            # Convierte las coordenadas normalizadas a píxeles\n",
    "            img_w, img_h = img.shape[1], img.shape[0]\n",
    "            x_pixel = int(xc * img_w - (bb_width * img_w) / 2)\n",
    "            y_pixel = int(yc * img_h - (bb_height * img_h) / 2)\n",
    "            w_pixel = int(bb_width * img_w)\n",
    "            h_pixel = int(bb_height * img_h)\n",
    "\n",
    "            # Crea el rectángulo\n",
    "            patch = patches.Rectangle((x_pixel, y_pixel), w_pixel, h_pixel,\n",
    "                                      linewidth=1, edgecolor='r', facecolor='none')\n",
    "\n",
    "            axs.imshow(img)\n",
    "            axs.add_patch(patch)\n",
    "            axs.axis('off')\n",
    "            axs.set_title(f'{sp_type}')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cac4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directorio que contiene los archivos de texto y las imágenes\n",
    "data_directory = \"C:\\\\Users\\\\cvasquezv\\\\Proyecto-vigor-defoliacion\\\\working\\\\data\\\\train\"\n",
    "\n",
    "# Visualizar imágenes aleatorias con bounding boxes\n",
    "display_images_with_rectangles(data_directory, rows=5, cols=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adac176",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df , data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abde7697",
   "metadata": {},
   "source": [
    "## Yolo v8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e69402a-829c-4e97-9ab4-9691d67c61b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U opencv-python\n",
    "#!conda install -c conda-forge opencv=4.9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5329f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ultralytics in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (8.0.220)\n",
      "Collecting ultralytics\n",
      "  Downloading ultralytics-8.1.11-py3-none-any.whl.metadata (40 kB)\n",
      "     ---------------------------------------- 0.0/40.2 kB ? eta -:--:--\n",
      "     ---------- ----------------------------- 10.2/40.2 kB ? eta -:--:--\n",
      "     ----------------------------- -------- 30.7/40.2 kB 262.6 kB/s eta 0:00:01\n",
      "     -------------------------------------- 40.2/40.2 kB 318.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from ultralytics) (3.8.2)\n",
      "Requirement already satisfied: numpy>=1.22.2 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from ultralytics) (1.26.2)\n",
      "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from ultralytics) (4.8.1.78)\n",
      "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from ultralytics) (10.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from ultralytics) (6.0.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from ultralytics) (2.31.0)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from ultralytics) (1.11.4)\n",
      "Requirement already satisfied: torch>=1.8.0 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from ultralytics) (2.1.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from ultralytics) (0.16.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from ultralytics) (4.66.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: thop>=0.1.1 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from ultralytics) (0.1.1.post2209072238)\n",
      "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from ultralytics) (2.1.3)\n",
      "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from ultralytics) (0.13.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\cvasquezv\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.10.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.45.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (23.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\cvasquezv\\appdata\\roaming\\python\\python310\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cvasquezv\\appdata\\roaming\\python\\python310\\site-packages (from requests>=2.23.0->ultralytics) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2023.11.17)\n",
      "Requirement already satisfied: filelock in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2023.10.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
      "Requirement already satisfied: six in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Downloading ultralytics-8.1.11-py3-none-any.whl (709 kB)\n",
      "   ---------------------------------------- 0.0/709.5 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 41.0/709.5 kB 991.0 kB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 81.9/709.5 kB 919.0 kB/s eta 0:00:01\n",
      "   ---- ---------------------------------- 81.9/709.5 kB 919.0 kB/s eta 0:00:01\n",
      "   ----- --------------------------------- 92.2/709.5 kB 655.4 kB/s eta 0:00:01\n",
      "   ------- ------------------------------ 143.4/709.5 kB 655.8 kB/s eta 0:00:01\n",
      "   ---------- --------------------------- 204.8/709.5 kB 778.2 kB/s eta 0:00:01\n",
      "   ------------- ------------------------ 245.8/709.5 kB 885.4 kB/s eta 0:00:01\n",
      "   ------------- ------------------------ 245.8/709.5 kB 885.4 kB/s eta 0:00:01\n",
      "   ------------- ------------------------ 245.8/709.5 kB 885.4 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 276.5/709.5 kB 630.9 kB/s eta 0:00:01\n",
      "   -------------- ----------------------- 276.5/709.5 kB 630.9 kB/s eta 0:00:01\n",
      "   ---------------- --------------------- 307.2/709.5 kB 542.9 kB/s eta 0:00:01\n",
      "   ------------------- ------------------ 358.4/709.5 kB 602.4 kB/s eta 0:00:01\n",
      "   -------------------------- ----------- 491.5/709.5 kB 770.0 kB/s eta 0:00:01\n",
      "   ----------------------------- -------- 553.0/709.5 kB 807.6 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 583.7/709.5 kB 815.7 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 593.9/709.5 kB 795.2 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 593.9/709.5 kB 795.2 kB/s eta 0:00:01\n",
      "   ------------------------------- ------ 593.9/709.5 kB 795.2 kB/s eta 0:00:01\n",
      "   -------------------------------- ----- 614.4/709.5 kB 666.8 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 675.8/709.5 kB 709.6 kB/s eta 0:00:01\n",
      "   ------------------------------------ - 675.8/709.5 kB 709.6 kB/s eta 0:00:01\n",
      "   -------------------------------------- 709.5/709.5 kB 688.2 kB/s eta 0:00:00\n",
      "Installing collected packages: ultralytics\n",
      "  Attempting uninstall: ultralytics\n",
      "    Found existing installation: ultralytics 8.0.220\n",
      "    Uninstalling ultralytics-8.0.220:\n",
      "      Successfully uninstalled ultralytics-8.0.220\n",
      "Successfully installed ultralytics-8.1.11\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1c3aa-faee-40b8-8e47-9ab46799d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --upgrade pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0221e07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/cuXXX/torch_stable.html\n",
      "Requirement already satisfied: torch in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: torchvision in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (0.16.1)\n",
      "Collecting torchaudio\n",
      "  Downloading torchaudio-2.2.0-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: requests in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from torchvision) (10.1.0)\n",
      "INFO: pip is looking at multiple versions of torchaudio to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading torchaudio-2.1.2-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
      "  Downloading torchaudio-2.1.1-cp310-cp310-win_amd64.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->torchvision) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\cvasquezv\\appdata\\roaming\\python\\python310\\site-packages (from requests->torchvision) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->torchvision) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\cvasquezv\\anaconda3\\envs\\myenv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Downloading torchaudio-2.1.1-cp310-cp310-win_amd64.whl (2.3 MB)\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/2.3 MB 653.6 kB/s eta 0:00:04\n",
      "   -- ------------------------------------- 0.1/2.3 MB 1.4 MB/s eta 0:00:02\n",
      "   ---- ----------------------------------- 0.3/2.3 MB 1.9 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.3/2.3 MB 1.5 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.5/2.3 MB 2.0 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 0.7/2.3 MB 2.2 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.9/2.3 MB 2.6 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.0/2.3 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.1/2.3 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.1/2.3 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.1/2.3 MB 2.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 1.3/2.3 MB 2.3 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 1.6/2.3 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 1.7/2.3 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 1.9/2.3 MB 2.8 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 2.1/2.3 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 2.2/2.3 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.3/2.3 MB 2.8 MB/s eta 0:00:00\n",
      "Installing collected packages: torchaudio\n",
      "Successfully installed torchaudio-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install torch torchvision torchaudio -f https://download.pytorch.org/whl/cuXXX/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43d27137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K\n",
      "\u001b[2K\n",
      "Ultralytics YOLOv8.1.11 🚀 Python-3.10.13 torch-2.1.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "Setup complete ✅ (8 CPUs, 39.7 GB RAM, 398.2/476.3 GB disk)\n",
      "\n",
      "OS                  Windows-10-10.0.19045-SP0\n",
      "Environment         Windows\n",
      "Python              3.10.13\n",
      "Install             pip\n",
      "RAM                 39.73 GB\n",
      "CPU                 11th Gen Intel Core(TM) i7-1165G7 2.80GHz\n",
      "CUDA                None\n",
      "\n",
      "matplotlib          ✅ 3.8.2>=3.3.0\n",
      "numpy               ✅ 1.26.2>=1.22.2\n",
      "opencv-python       ✅ 4.8.1.78>=4.6.0\n",
      "pillow              ✅ 10.1.0>=7.1.2\n",
      "pyyaml              ✅ 6.0.1>=5.3.1\n",
      "requests            ✅ 2.31.0>=2.23.0\n",
      "scipy               ✅ 1.11.4>=1.4.1\n",
      "torch               ✅ 2.1.1>=1.8.0\n",
      "torchvision         ✅ 0.16.1>=0.9.0\n",
      "tqdm                ✅ 4.66.1>=4.64.0\n",
      "psutil              ✅ 5.9.0\n",
      "py-cpuinfo          ✅ 9.0.0\n",
      "thop                ✅ 0.1.1-2209072238>=0.1.1\n",
      "pandas              ✅ 2.1.3>=1.1.4\n",
      "seaborn             ✅ 0.13.0>=0.11.0\n"
     ]
    }
   ],
   "source": [
    "import ultralytics\n",
    "from IPython import display\n",
    "display.clear_output()\n",
    "!yolo checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0efead58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da005c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model\n",
    "model = YOLO(\"yolov8x.pt\")  # load a pretrained model (recommended for training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa789568",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "imgsz = 640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86663ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the model\n",
    "results = model.train(data=\"C:/Users/cvasquezv/Proyecto-vigor-defoliacion/working/data.yaml\",\n",
    "                      epochs=250,\n",
    "                      batch = batch_size,\n",
    "                      imgsz=imgsz)  # train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f088d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta al directorio que contiene las imágenes\n",
    "ruta_imagenes = \"working\\\\data\\\\test\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f091e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener la lista de archivos de imagen en el directorio\n",
    "image_files = [f for f in os.listdir(ruta_imagenes) if f.endswith((\".jpg\", \".jpeg\", \".png\"))]\n",
    "\n",
    "# Construir rutas completas de las imágenes\n",
    "image_paths = [os.path.join(ruta_imagenes, file) for file in image_files]\n",
    "\n",
    "# Ahora, image_paths contiene las rutas completas de las imágenes\n",
    "print(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96d10e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Lista de rutas a las nuevas imágenes\n",
    "#image_paths = ['working\\\\data\\\\test\\\\002b37ac08e1.jpg',\n",
    "#               'working\\\\data\\\\test\\\\002da3868a1a.jpg',\n",
    "#               'working\\\\data\\\\test\\\\0036742ac4ec.jpg']\n",
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0ac907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar las imágenes\n",
    "images = [Image.open(path) for path in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e887ccc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Realizar inferencias en las imágenes\n",
    "inference = model.predict(images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88d5b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff940bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dir(inference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ceaf519",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Itera a través de las rutas de las imágenes\n",
    "for image_path in image_paths:\n",
    "    # Carga la imagen\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Preprocesa la imagen según sea necesario\n",
    "    \n",
    "    # Realiza la inferencia\n",
    "    results = model.predict(image,\n",
    "                            save=True,\n",
    "                            save_crop=True,\n",
    "                            save_txt=True,\n",
    "                            imgsz=640,\n",
    "                            conf=0.1,\n",
    "                            max_det=300)\n",
    "    \n",
    "    # Postprocesa los resultados y visualiza o almacena la salida según tus necesidades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b338642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Itera a través de las rutas de las imágenes\n",
    "for image_path in image_paths:\n",
    "    # Carga la imagen\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Realiza la inferencia\n",
    "    results = model.predict(image)\n",
    "    \n",
    "    # Show the results for this image\n",
    "    for r in results:\n",
    "        im_array = r.plot()  # plot a BGR numpy array of predictions\n",
    "        im = Image.fromarray(im_array[..., ::-1])  # RGB PIL image\n",
    "        im.show()  # show image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc61d25-c60a-473b-a256-9d330b466b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ultralytics==8.0.175\n",
    "#!pip install ultralytics\n",
    "#!pip install ultralytics==8.0.213"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440e1193-26d3-4840-9a20-787fbc418446",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ruamel.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abc9397-55f4-4a5c-ba8c-e8d891c0326e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda update -n myenv opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd6c8529",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8.0\n"
     ]
    }
   ],
   "source": [
    "#import sys\n",
    "#sys.path.append(r'C:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\Lib\\site-packages')\n",
    "\n",
    "import cv2\n",
    "print(cv2.__version__)\n",
    "\n",
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2164c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the YOLOv8 model\n",
    "model = YOLO(\"G:/Mi unidad/Python/Proyecto-vigor-defoliacion/runs/detect/train8/weights/best.pt/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc062921",
   "metadata": {},
   "source": [
    "## Ver inferencias del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6225ed92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the video file\n",
    "video_path = \"8.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "screen_width = 800  # Reemplaza con la resolución de tu pantalla\n",
    "screen_height = 600  # Reemplaza con la resolución de tu pantalla\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75554ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener las propiedades del video (ancho, alto, fps)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Porcentaje de reducción de tamaño de la ventana\n",
    "reduction_percentage = 40\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 inference on the original frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Reduce the size of the frame for display\n",
    "        small_frame = cv2.resize(annotated_frame, (int(width * reduction_percentage / 100), int(height * reduction_percentage / 100)))\n",
    "\n",
    "        # Display the annotated frame with reduced window size\n",
    "        cv2.imshow(\"YOLOv8 Inference\", small_frame)\n",
    "\n",
    "        # Reduce the delay for higher FPS\n",
    "        cv2.waitKey(int(1000 / fps))  # Set a delay based on the original video's FPS\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1f8570",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0730f8b3",
   "metadata": {},
   "source": [
    "## Guardar video con conteo y dimensiones reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b360d3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Error reading video file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m video_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvideo1.mp4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      8\u001b[0m cap \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mVideoCapture(video_path)\n\u001b[1;32m----> 9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m cap\u001b[38;5;241m.\u001b[39misOpened(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError reading video file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     10\u001b[0m w, h, fps \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m(cap\u001b[38;5;241m.\u001b[39mget(x)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m (cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FRAME_WIDTH, cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FRAME_HEIGHT, cv2\u001b[38;5;241m.\u001b[39mCAP_PROP_FPS))\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Percentage reduction of window size\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Error reading video file"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from ultralytics.solutions import object_counter\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"video1.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Percentage reduction of window size\n",
    "reduction_percentage = 40\n",
    "\n",
    "# Define region points in a vertical orientation\n",
    "region_points = [(w*0.40, 0), (w*0.40, h), \n",
    "                 (w*0.50, h), (w*0.50, 0)]  # Ajusta las coordenadas según tus preferencias\n",
    "\n",
    "classes_to_count = [0, 1]\n",
    "\n",
    "# Video writer\n",
    "video_writer = cv2.VideoWriter(\"object_counting_output.avi\",\n",
    "                       cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                       fps,\n",
    "                       (w, h))\n",
    "\n",
    "\n",
    "# Init Object Counter\n",
    "counter = object_counter.ObjectCounter()\n",
    "# Ejemplo de reinicio manual del contador\n",
    "counter.counter_variable = 0\n",
    "\n",
    "counter.set_args(view_img=False,\n",
    "                 reg_pts=region_points,\n",
    "                 #view_in_counts = True,\n",
    "                 #view_out_counts = True,\n",
    "                 classes_names=model.model.names,\n",
    "                 draw_tracks=True\n",
    "                 )\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "    if not success:\n",
    "        print(\"El frame del video está vacío o el procesamiento del video se ha completado correctamente.\")\n",
    "        break\n",
    "    tracks = model.track(im0, persist=True, show=False, classes=classes_to_count)\n",
    "\n",
    "    # Inicia el conteo y obtén el número de objetos contados por clase\n",
    "    counts_by_class = counter.start_counting(im0, tracks)\n",
    "\n",
    "    # Reduce el tamaño de la imagen para su visualización\n",
    "    small_frame = cv2.resize(im0, (int(w * reduction_percentage / 100), int(h * reduction_percentage / 100)))\n",
    "\n",
    "    # Escribe el frame en el archivo de video\n",
    "    video_writer.write(im0)\n",
    "\n",
    "    # Muestra la imagen reducida\n",
    "    cv2.imshow(\"Object Counting\", small_frame)\n",
    "\n",
    "    # Rompe el bucle si se presiona 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bfba31",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce219c3",
   "metadata": {},
   "source": [
    "## Conteo y tracker multiclase con Yolov8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c8fb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.solutions import object_counter\n",
    "import csv\n",
    "\n",
    "\n",
    "model = YOLO(\"G:/Mi unidad/Python/Proyecto-vigor-defoliacion/runs/detect/train8/weights/best.pt/\")\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"8.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Percentage reduction of window size\n",
    "reduction_percentage = 40\n",
    "\n",
    "# Define region points in a vertical orientation\n",
    "region_points = [(w*0.40, 0), (w*0.40, h), \n",
    "                 (w*0.50, h), (w*0.50, 0)]  # Ajusta las coordenadas según tus preferencias\n",
    "\n",
    "\n",
    "classes_to_count = [0, 1]\n",
    "\n",
    "# Clases a contar (Dead y Alive)\n",
    "classes_to_count_dead = [0]  # Reemplaza con el índice correcto de la clase \"Dead\"\n",
    "classes_to_count_alive = [1]  # Reemplaza con el índice correcto de la clase \"Alive\"\n",
    "\n",
    "# Video writer\n",
    "video_writer = cv2.VideoWriter(\"object_counting_multiclass.avi\",\n",
    "                       cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                       fps,\n",
    "                       (w, h))\n",
    "\n",
    "# Init Object Counters for each class\n",
    "counter_dead = object_counter.ObjectCounter()\n",
    "counter_dead.counter_variable = 0\n",
    "counter_dead.set_args(view_img=False,\n",
    "                      view_in_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia adentro\n",
    "                      view_out_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia afuera\n",
    "                      reg_pts=region_points,\n",
    "                      classes_names=model.model.names,\n",
    "                      draw_tracks=True)\n",
    "\n",
    "counter_alive = object_counter.ObjectCounter()\n",
    "counter_alive.counter_variable = 0\n",
    "counter_alive.set_args(view_img=False,\n",
    "                       view_in_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia adentro\n",
    "                       view_out_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia afuera\n",
    "                       reg_pts=region_points,\n",
    "                       classes_names=model.model.names,\n",
    "                       draw_tracks=True)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "    if not success:\n",
    "        print(\"El frame del video está vacío o el procesamiento del video se ha completado correctamente.\")\n",
    "        break\n",
    "\n",
    "    # Inicia el conteo y obtén el número de objetos contados por clase\n",
    "    #tracks_dead = model.track(im0, persist=True, show=False, classes=classes_to_count_dead)\n",
    "    #counts_dead = counter_dead.start_counting(im0, tracks_dead)\n",
    "\n",
    "    #tracks_alive = model.track(im0, persist=True, show=False, classes=classes_to_count_alive)\n",
    "    #counts_alive = counter_alive.start_counting(im0, tracks_alive)\n",
    "\n",
    "    # Create a dictionary to map class labels to counters\n",
    "    counters_by_class = {\"Dead\": counter_dead, \"Alive\": counter_alive}\n",
    "\n",
    "    # Inicia el conteo y obtén el número de objetos contados por clase\n",
    "    tracks_dead = model.track(im0, persist=True, show=False, classes=classes_to_count_dead)\n",
    "    counts_dead = counters_by_class[\"Dead\"].start_counting(im0, tracks_dead)\n",
    "\n",
    "    tracks_alive = model.track(im0, persist=True, show=False, classes=classes_to_count_alive)\n",
    "    counts_alive = counters_by_class[\"Alive\"].start_counting(im0, tracks_alive)\n",
    "\n",
    "    # Dead Count\n",
    "    text_dead = f'Dead: {counter_dead.out_counts}'\n",
    "    font_dead = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale_dead = 2\n",
    "    font_thickness_dead = 4\n",
    "    font_color_dead = (0, 0, 255)\n",
    "    background_color_dead = (0, 0, 0)\n",
    "\n",
    "    # Get the size of the text\n",
    "    (text_width_dead, text_height_dead), _ = cv2.getTextSize(text_dead, font_dead, font_scale_dead, font_thickness_dead)\n",
    "\n",
    "    # Set the position for the text\n",
    "    text_position_dead = (10, 60)\n",
    "\n",
    "    # Create a black background for the text\n",
    "    background_rect_dead = (text_position_dead[0], text_position_dead[1] - text_height_dead, text_width_dead, text_height_dead)\n",
    "    cv2.rectangle(im0, background_rect_dead, background_color_dead, thickness=cv2.FILLED)\n",
    "\n",
    "    # Put the text on the image\n",
    "    cv2.putText(im0, text_dead, text_position_dead, font_dead, font_scale_dead, font_color_dead, font_thickness_dead, cv2.LINE_AA)\n",
    "\n",
    "    # Alive Count\n",
    "    text_alive = f'Alive: {counter_alive.out_counts}'\n",
    "    font_alive = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale_alive = 2\n",
    "    font_thickness_alive = 4\n",
    "    font_color_alive = (0, 255, 0)\n",
    "    background_color_alive = (0, 0, 0)\n",
    "\n",
    "    # Get the size of the text\n",
    "    (text_width_alive, text_height_alive), _ = cv2.getTextSize(text_alive, font_alive, font_scale_alive, font_thickness_alive)\n",
    "\n",
    "    # Set the position for the text\n",
    "    text_position_alive = (10, 110)\n",
    "\n",
    "    # Create a black background for the text\n",
    "    background_rect_alive = (text_position_alive[0], text_position_alive[1] - text_height_alive, text_width_alive, text_height_alive)\n",
    "    cv2.rectangle(im0, background_rect_alive, background_color_alive, thickness=cv2.FILLED)\n",
    "\n",
    "    # Put the text on the image\n",
    "    cv2.putText(im0, text_alive, text_position_alive, font_alive, font_scale_alive, font_color_alive, font_thickness_alive, cv2.LINE_AA)\n",
    "\n",
    "    # Añade el texto con los conteos a la imagen\n",
    "    #cv2.putText(im0, f'Dead: {counter_dead.out_counts}', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "    #cv2.putText(im0, f'Alive: {counter_alive.out_counts}', (10, 70), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "    # Reduce el tamaño de la imagen para su visualización\n",
    "    small_frame = cv2.resize(im0, (int(w * reduction_percentage / 100), int(h * reduction_percentage / 100)))\n",
    "    \n",
    "    # Escribe el frame en el archivo de video\n",
    "    video_writer.write(im0)\n",
    "\n",
    "    # Muestra la imagen reducida\n",
    "    cv2.imshow(\"Object Counting\", small_frame)\n",
    "\n",
    "    # Rompe el bucle si se presiona 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf4194",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9261a182",
   "metadata": {},
   "source": [
    "## Conteo, Tracker y Registro de múltiples clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "591c83da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.11 🚀 Python-3.10.13 torch-2.1.1+cpu CPU (11th Gen Intel Core(TM) i7-1165G7 2.80GHz)\n",
      "Setup complete ✅ (8 CPUs, 39.7 GB RAM, 399.0/476.3 GB disk)\n"
     ]
    }
   ],
   "source": [
    "from IPython import display\n",
    "display.clear_output()\n",
    " \n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98e524a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7cdd87fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.solutions import object_counter\n",
    "import csv\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "model = YOLO(\"G:/Mi unidad/Python/Proyecto-deteccion-bayas/runs/detect/train4/weights/best.pt/\")\n",
    "# model.to('cpu')\n",
    "\n",
    "# Export the model\n",
    "# model.export(format='openvino')  # creates 'yolov8n_openvino_model/'\n",
    "\n",
    "# Load the exported OpenVINO model\n",
    "# ov_model = YOLO('G:/Mi unidad/Python/Proyecto-deteccion-bayas/runs/detect/train4/weights/best.onnx/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62b20f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region Counter Initiated.\n",
      "Region Counter Initiated.\n",
      "Region Counter Initiated.\n",
      "Region Counter Initiated.\n",
      "Region Counter Initiated.\n",
      "Region Counter Initiated.\n",
      "Region Counter Initiated.\n",
      "Region Counter Initiated.\n",
      "Region Counter Initiated.\n",
      "\n",
      "0: 640x384 (no detections), 976.7ms\n",
      "Speed: 10.1ms preprocess, 976.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 952.1ms\n",
      "Speed: 1.4ms preprocess, 952.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Set fruit, 951.2ms\n",
      "Speed: 0.0ms preprocess, 951.2ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 949.6ms\n",
      "Speed: 0.0ms preprocess, 949.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 942.1ms\n",
      "Speed: 8.0ms preprocess, 942.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 897.7ms\n",
      "Speed: 0.0ms preprocess, 897.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 931.3ms\n",
      "Speed: 1.0ms preprocess, 931.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 964.6ms\n",
      "Speed: 2.3ms preprocess, 964.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 942.5ms\n",
      "Speed: 0.0ms preprocess, 942.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1250.0ms\n",
      "Speed: 1.0ms preprocess, 1250.0ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1253.6ms\n",
      "Speed: 1.0ms preprocess, 1253.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Set fruit, 1015.9ms\n",
      "Speed: 2.0ms preprocess, 1015.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1490.6ms\n",
      "Speed: 0.0ms preprocess, 1490.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1735.3ms\n",
      "Speed: 3.1ms preprocess, 1735.3ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1273.3ms\n",
      "Speed: 2.0ms preprocess, 1273.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Pink fruit, 1133.5ms\n",
      "Speed: 0.0ms preprocess, 1133.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1053.4ms\n",
      "Speed: 0.0ms preprocess, 1053.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1052.4ms\n",
      "Speed: 1.0ms preprocess, 1052.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 976.6ms\n",
      "Speed: 2.0ms preprocess, 976.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1000.1ms\n",
      "Speed: 0.0ms preprocess, 1000.1ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Set fruit, 976.2ms\n",
      "Speed: 0.0ms preprocess, 976.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 982.7ms\n",
      "Speed: 1.5ms preprocess, 982.7ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 972.9ms\n",
      "Speed: 2.0ms preprocess, 972.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1028.0ms\n",
      "Speed: 0.0ms preprocess, 1028.0ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 2 Pink fruits, 1047.3ms\n",
      "Speed: 0.0ms preprocess, 1047.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1076.3ms\n",
      "Speed: 0.0ms preprocess, 1076.3ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1089.9ms\n",
      "Speed: 0.0ms preprocess, 1089.9ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1048.2ms\n",
      "Speed: 1.0ms preprocess, 1048.2ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1033.4ms\n",
      "Speed: 1.7ms preprocess, 1033.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Set fruit, 1044.5ms\n",
      "Speed: 0.0ms preprocess, 1044.5ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1002.9ms\n",
      "Speed: 8.0ms preprocess, 1002.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 979.0ms\n",
      "Speed: 0.0ms preprocess, 979.0ms inference, 3.5ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 996.8ms\n",
      "Speed: 0.0ms preprocess, 996.8ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 964.4ms\n",
      "Speed: 0.0ms preprocess, 964.4ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1066.6ms\n",
      "Speed: 3.4ms preprocess, 1066.6ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1073.7ms\n",
      "Speed: 2.0ms preprocess, 1073.7ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 995.5ms\n",
      "Speed: 1.0ms preprocess, 995.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1135.5ms\n",
      "Speed: 0.0ms preprocess, 1135.5ms inference, 0.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 1 Set fruit, 1092.1ms\n",
      "Speed: 8.1ms preprocess, 1092.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n",
      "0: 640x384 (no detections), 1215.7ms\n",
      "Speed: 2.0ms preprocess, 1215.7ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 384)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 150\u001b[0m\n\u001b[0;32m    146\u001b[0m tracks_3 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrack(im0, persist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, classes\u001b[38;5;241m=\u001b[39mclasses_to_count_3,\n\u001b[0;32m    147\u001b[0m                             conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m, iou \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    148\u001b[0m counts_3 \u001b[38;5;241m=\u001b[39m counters_by_class[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGreen fruit\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstart_counting(im0, tracks_3)\n\u001b[1;32m--> 150\u001b[0m tracks_4 \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpersist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclasses_to_count_4\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mconf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miou\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    152\u001b[0m counts_4 \u001b[38;5;241m=\u001b[39m counters_by_class[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPartially green fruit\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstart_counting(im0, tracks_4)\n\u001b[0;32m    154\u001b[0m tracks_5 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mtrack(im0, persist\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, show\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, classes\u001b[38;5;241m=\u001b[39mclasses_to_count_5,\n\u001b[0;32m    155\u001b[0m                             conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m, iou \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.2\u001b[39m, verbose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ultralytics\\engine\\model.py:439\u001b[0m, in \u001b[0;36mModel.track\u001b[1;34m(self, source, stream, persist, **kwargs)\u001b[0m\n\u001b[0;32m    437\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconf\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconf\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m0.1\u001b[39m  \u001b[38;5;66;03m# ByteTrack-based method needs low confidence predictions as input\u001b[39;00m\n\u001b[0;32m    438\u001b[0m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrack\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(source\u001b[38;5;241m=\u001b[39msource, stream\u001b[38;5;241m=\u001b[39mstream, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ultralytics\\engine\\model.py:406\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[0;32m    405\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[1;32m--> 406\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ultralytics\\engine\\predictor.py:204\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[1;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    203\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\utils\\_contextlib.py:35\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m---> 35\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ultralytics\\engine\\predictor.py:283\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[1;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    281\u001b[0m \u001b[38;5;66;03m# Inference\u001b[39;00m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m--> 283\u001b[0m     preds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minference(im, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed:\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m [preds] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m preds  \u001b[38;5;66;03m# yield embedding tensors\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ultralytics\\engine\\predictor.py:140\u001b[0m, in \u001b[0;36mBasePredictor.inference\u001b[1;34m(self, im, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runs inference on a given image using the specified model and arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    135\u001b[0m visualize \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    136\u001b[0m     increment_path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave_dir \u001b[38;5;241m/\u001b[39m Path(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mstem, mkdir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mvisualize \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msource_type\u001b[38;5;241m.\u001b[39mtensor)\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    139\u001b[0m )\n\u001b[1;32m--> 140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39maugment, visualize\u001b[38;5;241m=\u001b[39mvisualize, embed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39membed, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ultralytics\\nn\\autobackend.py:384\u001b[0m, in \u001b[0;36mAutoBackend.forward\u001b[1;34m(self, im, augment, visualize, embed)\u001b[0m\n\u001b[0;32m    381\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# torch BCHW to numpy BHWC shape(1,320,192,3)\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpt \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnn_module:  \u001b[38;5;66;03m# PyTorch\u001b[39;00m\n\u001b[1;32m--> 384\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maugment\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjit:  \u001b[38;5;66;03m# TorchScript\u001b[39;00m\n\u001b[0;32m    386\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(im)\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ultralytics\\nn\\tasks.py:80\u001b[0m, in \u001b[0;36mBaseModel.forward\u001b[1;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):  \u001b[38;5;66;03m# for cases of training and validating while training.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m---> 80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ultralytics\\nn\\tasks.py:98\u001b[0m, in \u001b[0;36mBaseModel.predict\u001b[1;34m(self, x, profile, visualize, augment, embed)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m augment:\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_predict_augment(x)\n\u001b[1;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_predict_once\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvisualize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ultralytics\\nn\\tasks.py:119\u001b[0m, in \u001b[0;36mBaseModel._predict_once\u001b[1;34m(self, x, profile, visualize, embed)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m profile:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_profile_one_layer(m, x, dt)\n\u001b[1;32m--> 119\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# run\u001b[39;00m\n\u001b[0;32m    120\u001b[0m y\u001b[38;5;241m.\u001b[39mappend(x \u001b[38;5;28;01mif\u001b[39;00m m\u001b[38;5;241m.\u001b[39mi \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# save output\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m visualize:\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:221\u001b[0m, in \u001b[0;36mC2f.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 221\u001b[0m \u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:221\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Forward pass through C2f layer.\"\"\"\u001b[39;00m\n\u001b[0;32m    220\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x)\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m--> 221\u001b[0m y\u001b[38;5;241m.\u001b[39mextend(\u001b[43mm\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mm)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(torch\u001b[38;5;241m.\u001b[39mcat(y, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ultralytics\\nn\\modules\\block.py:331\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m    330\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"'forward()' applies the YOLO FPN to input data.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 331\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv2\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv1(x))\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\ultralytics\\nn\\modules\\conv.py:54\u001b[0m, in \u001b[0;36mConv.forward_fuse\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_fuse\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     53\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Perform transposed convolution of 2D data.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\cvasquezv\\Anaconda3\\envs\\myenv\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Open the video file\n",
    "video_path = \"video1.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Percentage reduction of window size\n",
    "reduction_percentage = 60\n",
    "\n",
    "# Define region points in a vertical orientation\n",
    "region_points = [(w*0.40, 0), (w*0.40, h), \n",
    "                 (w*0.50, h), (w*0.50, 0)]  # Ajusta las coordenadas según tus preferencias\n",
    "\n",
    "classes_to_count = [0, 1, 2, 3, 4, 5, 6, 7]\n",
    "\n",
    "# Clases a contar \n",
    "classes_to_count_0 = [0]  # Reemplaza con el índice correcto de la clase \"Closed flower\"\n",
    "classes_to_count_1 = [1]  # Reemplaza con el índice correcto de la clase \"Open flower\"\n",
    "classes_to_count_2 = [2]  # Reemplaza con el índice correcto de la clase \"Set fruit\"\n",
    "classes_to_count_3 = [3]  # Reemplaza con el índice correcto de la clase \"Green fruit\"\n",
    "classes_to_count_4 = [4]  # Reemplaza con el índice correcto de la clase \"Partially green fruit\"\n",
    "classes_to_count_5 = [5]  # Reemplaza con el índice correcto de la clase \"Blushing fruit\"\n",
    "classes_to_count_6 = [6]  # Reemplaza con el índice correcto de la clase \"Pink fruit\"\n",
    "classes_to_count_7 = [7]  # Reemplaza con el índice correcto de la clase \"Blue fruit\"\n",
    "\n",
    "# Video writer\n",
    "video_writer = cv2.VideoWriter(\"object_counting_multiclass.avi\",\n",
    "                       cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                       fps,\n",
    "                       (w, h))\n",
    "\n",
    "# Init Object Counters for each class\n",
    "counter_0 = object_counter.ObjectCounter()\n",
    "counter_0.counter_variable = 0\n",
    "counter_0.set_args(view_img=False,\n",
    "                      view_in_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia adentro\n",
    "                      view_out_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia afuera\n",
    "                      reg_pts=region_points,\n",
    "                      classes_names=model.names,\n",
    "                      draw_tracks=True)\n",
    "\n",
    "counter_1 = object_counter.ObjectCounter()\n",
    "counter_1.counter_variable = 0\n",
    "counter_1.set_args(view_img=False,\n",
    "                       view_in_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia adentro\n",
    "                       view_out_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia afuera\n",
    "                       reg_pts=region_points,\n",
    "                       classes_names=model.names,\n",
    "                       draw_tracks=True)\n",
    "\n",
    "counter_2 = object_counter.ObjectCounter()\n",
    "counter_2.counter_variable = 0\n",
    "counter_2.set_args(view_img=False,\n",
    "                       view_in_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia adentro\n",
    "                       view_out_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia afuera\n",
    "                       reg_pts=region_points,\n",
    "                       classes_names=model.names,\n",
    "                       draw_tracks=True)\n",
    "\n",
    "counter_3 = object_counter.ObjectCounter()\n",
    "counter_3.counter_variable = 0\n",
    "counter_3.set_args(view_img=False,\n",
    "                       view_in_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia adentro\n",
    "                       view_out_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia afuera\n",
    "                       reg_pts=region_points,\n",
    "                       classes_names=model.names,\n",
    "                       draw_tracks=True)\n",
    "\n",
    "counter_4 = object_counter.ObjectCounter()\n",
    "counter_4.counter_variable = 0\n",
    "counter_4.set_args(view_img=False,\n",
    "                       view_in_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia adentro\n",
    "                       view_out_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia afuera\n",
    "                       reg_pts=region_points,\n",
    "                       classes_names=model.names,\n",
    "                       draw_tracks=True)\n",
    "\n",
    "counter_5 = object_counter.ObjectCounter()\n",
    "counter_5.counter_variable = 0\n",
    "counter_5.set_args(view_img=False,\n",
    "                       view_in_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia adentro\n",
    "                       view_out_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia afuera\n",
    "                       reg_pts=region_points,\n",
    "                       classes_names=model.names,\n",
    "                       draw_tracks=True)\n",
    "\n",
    "counter_6 = object_counter.ObjectCounter()\n",
    "counter_6.counter_variable = 0\n",
    "counter_6.set_args(view_img=False,\n",
    "                       view_in_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia adentro\n",
    "                       view_out_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia afuera\n",
    "                       reg_pts=region_points,\n",
    "                       classes_names=model.names,\n",
    "                       draw_tracks=True)\n",
    "\n",
    "counter_7 = object_counter.ObjectCounter()\n",
    "counter_7.counter_variable = 0\n",
    "counter_7.set_args(view_img=False,\n",
    "                       view_in_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia adentro\n",
    "                       view_out_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia afuera\n",
    "                       reg_pts=region_points,\n",
    "                       classes_names=model.names,\n",
    "                       draw_tracks=True)\n",
    "\n",
    "counter_all = object_counter.ObjectCounter()\n",
    "counter_all.counter_variable = 0\n",
    "counter_all.set_args(view_img=False,\n",
    "                     view_in_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia adentro\n",
    "                     view_out_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia afuera\n",
    "                     reg_pts=region_points,\n",
    "                     classes_names=model.names,\n",
    "                     draw_tracks=False)\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "    if not success:\n",
    "        print(\"El frame del video está vacío o el procesamiento del video se ha completado correctamente.\")\n",
    "        break\n",
    "\n",
    "    # Create a dictionary to map class labels to counters\n",
    "    counters_by_class = {\n",
    "        \"Closed flower\": counter_0,\n",
    "        \"Open flower\": counter_1,\n",
    "        \"Set fruit\": counter_2,\n",
    "        \"Green fruit\": counter_3,\n",
    "        \"Partially green fruit\": counter_4,\n",
    "        \"Blushing fruit\": counter_5,\n",
    "        \"Pink fruit\": counter_6,\n",
    "        \"Blue fruit\": counter_7,\n",
    "        \"All\": counter_all\n",
    "    }\n",
    "    \n",
    "    # Inicia el conteo y obtén el número de objetos contados por clase\n",
    "    tracks_0 = model.track(im0, persist=True, show=False, classes=classes_to_count_0,\n",
    "                               conf = 0.3, iou = 0.9, verbose = True)\n",
    "    counts_0 = counters_by_class[\"Closed flower\"].start_counting(im0, tracks_0)\n",
    "\n",
    "    tracks_1 = model.track(im0, persist=True, show=False, classes=classes_to_count_1,\n",
    "                                conf = 0.3, iou = 0.9, verbose = True)\n",
    "    counts_1 = counters_by_class[\"Open flower\"].start_counting(im0, tracks_1)\n",
    "\n",
    "    tracks_2 = model.track(im0, persist=True, show=False, classes=classes_to_count_2,\n",
    "                                conf = 0.3, iou = 0.9, verbose = True)\n",
    "    counts_2 = counters_by_class[\"Set fruit\"].start_counting(im0, tracks_2)\n",
    "    \n",
    "    tracks_3 = model.track(im0, persist=True, show=False, classes=classes_to_count_3,\n",
    "                                conf = 0.3, iou = 0.9, verbose = True)\n",
    "    counts_3 = counters_by_class[\"Green fruit\"].start_counting(im0, tracks_3)\n",
    "\n",
    "    tracks_4 = model.track(im0, persist=True, show=False, classes=classes_to_count_4,\n",
    "                                conf = 0.3, iou = 0.9, verbose = True)\n",
    "    counts_4 = counters_by_class[\"Partially green fruit\"].start_counting(im0, tracks_4)\n",
    "\n",
    "    tracks_5 = model.track(im0, persist=True, show=False, classes=classes_to_count_5,\n",
    "                                conf = 0.3, iou = 0.9, verbose = True)\n",
    "    counts_5 = counters_by_class[\"Blushing fruit\"].start_counting(im0, tracks_5)\n",
    "\n",
    "    tracks_6 = model.track(im0, persist=True, show=False, classes=classes_to_count_6,\n",
    "                                conf = 0.3, iou = 0.9, verbose = True)\n",
    "    counts_6 = counters_by_class[\"Pink fruit\"].start_counting(im0, tracks_6)\n",
    "\n",
    "    tracks_7 = model.track(im0, persist=True, show=False, classes=classes_to_count_7,\n",
    "                                conf = 0.3, iou = 0.9, verbose = True)\n",
    "    counts_7 = counters_by_class[\"Blue fruit\"].start_counting(im0, tracks_7)\n",
    "\n",
    "    tracks_all = model.track(im0, persist=True, show=False,\n",
    "                                conf = 0.3, iou = 0.9, verbose = True)\n",
    "    counts_all = counters_by_class[\"All\"].start_counting(im0, tracks_all)\n",
    "\n",
    "    # Iniciar el conteo y obtener el número de objetos contados por clase\n",
    "\n",
    "    # Closed flower Count\n",
    "    text_0 = f'Closed flower: {counter_0.out_counts}'\n",
    "    font_0 = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale_0 = 2\n",
    "    font_thickness_0 = 4\n",
    "    font_color_0 = (173, 216, 230)\n",
    "    background_color_0 = (0, 0, 0)\n",
    "\n",
    "    # Get the size of the text\n",
    "    (text_width_0, text_height_0), _ = cv2.getTextSize(text_0, font_0, font_scale_0, font_thickness_0)\n",
    "\n",
    "    # Set the position for the text\n",
    "    text_position_0 = (10, 60)\n",
    "\n",
    "    # Create a black background for the text\n",
    "    background_rect_0 = (text_position_0[0], text_position_0[1] - text_height_0, text_width_0, text_height_0)\n",
    "    cv2.rectangle(im0, background_rect_0, background_color_0, thickness=cv2.FILLED)\n",
    "\n",
    "    # Put the text on the image\n",
    "    cv2.putText(im0, text_0, text_position_0, font_0, font_scale_0, font_color_0, font_thickness_0, cv2.LINE_AA)\n",
    "\n",
    "    # Open flower Count\n",
    "    text_1 = f'Open flower: {counter_1.out_counts}'\n",
    "    font_1 = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale_1 = 2\n",
    "    font_thickness_1 = 4\n",
    "    font_color_1 = (0, 100, 0)\n",
    "    background_color_1 = (0, 0, 0)\n",
    "\n",
    "    # Get the size of the text\n",
    "    (text_width_1, text_height_1), _ = cv2.getTextSize(text_1, font_1, font_scale_1, font_thickness_1)\n",
    "\n",
    "    # Set the position for the text\n",
    "    text_position_1 = (10, 110)\n",
    "\n",
    "    # Create a black background for the text\n",
    "    background_rect_1 = (text_position_1[0], text_position_1[1] - text_height_1, text_width_1, text_height_1)\n",
    "    cv2.rectangle(im0, background_rect_1, background_color_1, thickness=cv2.FILLED)\n",
    "\n",
    "    # Put the text on the image\n",
    "    cv2.putText(im0, text_1, text_position_1, font_1, font_scale_1, font_color_1, font_thickness_1, cv2.LINE_AA)\n",
    "\n",
    "    # Set fruit Count\n",
    "    text_2 = f'Set fruit: {counter_2.out_counts}'\n",
    "    font_2 = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale_2 = 2\n",
    "    font_thickness_2 = 4\n",
    "    font_color_2 = (128, 128, 0)\n",
    "    background_color_2 = (0, 0, 0)\n",
    "\n",
    "    # Get the size of the text\n",
    "    (text_width_2, text_height_2), _ = cv2.getTextSize(text_2, font_2, font_scale_2, font_thickness_2)\n",
    "\n",
    "    # Set the position for the text\n",
    "    text_position_2 = (10, 160)\n",
    "\n",
    "    # Create a black background for the text\n",
    "    background_rect_2 = (text_position_2[0], text_position_2[1] - text_height_2, text_width_2, text_height_2)\n",
    "    cv2.rectangle(im0, background_rect_2, background_color_2, thickness=cv2.FILLED)\n",
    "\n",
    "    # Put the text on the image\n",
    "    cv2.putText(im0, text_2, text_position_2, font_2, font_scale_2, font_color_2, font_thickness_2, cv2.LINE_AA)    \n",
    "\n",
    "    # Green Count\n",
    "    text_3 = f'Green: {counter_3.out_counts}'\n",
    "    font_3 = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale_3 = 2\n",
    "    font_thickness_3 = 4\n",
    "    font_color_3 = (0, 128, 0)\n",
    "    background_color_3 = (0, 0, 0)\n",
    "\n",
    "    # Get the size of the text\n",
    "    (text_width_3, text_height_3), _ = cv2.getTextSize(text_3, font_3, font_scale_3, font_thickness_3)\n",
    "\n",
    "    # Set the position for the text\n",
    "    text_position_3 = (10, 210)\n",
    "\n",
    "    # Create a black background for the text\n",
    "    background_rect_3 = (text_position_3[0], text_position_3[1] - text_height_3, text_width_3, text_height_3)\n",
    "    cv2.rectangle(im0, background_rect_3, background_color_3, thickness=cv2.FILLED)\n",
    "\n",
    "    # Put the text on the image\n",
    "    cv2.putText(im0, text_3, text_position_3, font_3, font_scale_3, font_color_3, font_thickness_3, cv2.LINE_AA)\n",
    "\n",
    "    # Partially green Count\n",
    "    text_4 = f'Partially green: {counter_4.out_counts}'\n",
    "    font_4 = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale_4 = 2\n",
    "    font_thickness_4 = 4\n",
    "    font_color_4 = (85, 107, 47)\n",
    "    background_color_4 = (0, 0, 0)\n",
    "\n",
    "    # Get the size of the text\n",
    "    (text_width_4, text_height_4), _ = cv2.getTextSize(text_4, font_4, font_scale_4, font_thickness_4)\n",
    "\n",
    "    # Set the position for the text\n",
    "    text_position_4 = (10, 260)\n",
    "\n",
    "    # Create a black background for the text\n",
    "    background_rect_4 = (text_position_4[0], text_position_4[1] - text_height_4, text_width_4, text_height_4)\n",
    "    cv2.rectangle(im0, background_rect_4, background_color_4, thickness=cv2.FILLED)\n",
    "\n",
    "    # Put the text on the image\n",
    "    cv2.putText(im0, text_4, text_position_4, font_4, font_scale_4, font_color_4, font_thickness_4, cv2.LINE_AA)\n",
    "\n",
    "    # Blushing Count\n",
    "    text_5 = f'Blushing: {counter_5.out_counts}'\n",
    "    font_5 = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale_5 = 2\n",
    "    font_thickness_5 = 4\n",
    "    font_color_5 = (255, 182, 193)\n",
    "    background_color_5 = (0, 0, 0)\n",
    "\n",
    "    # Get the size of the text\n",
    "    (text_width_5, text_height_5), _ = cv2.getTextSize(text_5, font_5, font_scale_5, font_thickness_5)\n",
    "\n",
    "    # Set the position for the text\n",
    "    text_position_5 = (10, 310)\n",
    "\n",
    "    # Create a black background for the text\n",
    "    background_rect_5 = (text_position_5[0], text_position_5[1] - text_height_5, text_width_5, text_height_5)\n",
    "    cv2.rectangle(im0, background_rect_5, background_color_5, thickness=cv2.FILLED)\n",
    "\n",
    "    # Put the text on the image\n",
    "    cv2.putText(im0, text_5, text_position_5, font_5, font_scale_5, font_color_5, font_thickness_5, cv2.LINE_AA)\n",
    "\n",
    "    # Pink Count\n",
    "    text_6 = f'Pink: {counter_6.out_counts}'\n",
    "    font_6 = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale_6 = 2\n",
    "    font_thickness_6 = 4\n",
    "    font_color_6 = (255, 105, 180)\n",
    "    background_color_6 = (0, 0, 0)\n",
    "\n",
    "    # Get the size of the text\n",
    "    (text_width_6, text_height_6), _ = cv2.getTextSize(text_6, font_6, font_scale_6, font_thickness_6)\n",
    "\n",
    "    # Set the position for the text\n",
    "    text_position_6 = (10, 360)\n",
    "\n",
    "    # Create a black background for the text\n",
    "    background_rect_6 = (text_position_6[0], text_position_6[1] - text_height_6, text_width_6, text_height_6)\n",
    "    cv2.rectangle(im0, background_rect_6, background_color_6, thickness=cv2.FILLED)\n",
    "\n",
    "    # Put the text on the image\n",
    "    cv2.putText(im0, text_6, text_position_6, font_6, font_scale_6, font_color_6, font_thickness_6, cv2.LINE_AA)\n",
    "\n",
    "    # Blue Count\n",
    "    text_7 = f'Blue: {counter_7.out_counts}'\n",
    "    font_7 = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale_7 = 2\n",
    "    font_thickness_7 = 4\n",
    "    font_color_7 = (139, 0, 0)\n",
    "    background_color_7 = (0, 0, 0)\n",
    "\n",
    "    # Get the size of the text\n",
    "    (text_width_7, text_height_7), _ = cv2.getTextSize(text_7, font_7, font_scale_7, font_thickness_7)\n",
    "\n",
    "    # Set the position for the text\n",
    "    text_position_7 = (10, 410)\n",
    "\n",
    "    # Create a black background for the text\n",
    "    background_rect_7 = (text_position_7[0], text_position_7[1] - text_height_7, text_width_7, text_height_7)\n",
    "    cv2.rectangle(im0, background_rect_7, background_color_7, thickness=cv2.FILLED)\n",
    "\n",
    "    # Put the text on the image\n",
    "    cv2.putText(im0, text_7, text_position_7, font_7, font_scale_7, font_color_7, font_thickness_7, cv2.LINE_AA)\n",
    "\n",
    "    # Guardar conteos en stream en un archivo Excel\n",
    "    data_0 = {'id': counter_0.counting_list, 'class': 'Closed flower'}\n",
    "    df_0 = pd.DataFrame(data_0)\n",
    "\n",
    "    data_1 = {'id': counter_1.counting_list, 'class': 'Open flower'}\n",
    "    df_1 = pd.DataFrame(data_1)\n",
    "\n",
    "    data_2 = {'id': counter_2.counting_list, 'class': 'Set fruit'}\n",
    "    df_2 = pd.DataFrame(data_2)\n",
    "\n",
    "    data_3 = {'id': counter_3.counting_list, 'class': 'Green'}\n",
    "    df_3 = pd.DataFrame(data_3)\n",
    "\n",
    "    data_4 = {'id': counter_4.counting_list, 'class': 'Partially green'}\n",
    "    df_4 = pd.DataFrame(data_4)\n",
    "\n",
    "    data_5 = {'id': counter_5.counting_list, 'class': 'Blushing'}\n",
    "    df_5 = pd.DataFrame(data_5)\n",
    "\n",
    "    data_6 = {'id': counter_6.counting_list, 'class': 'Pink'}\n",
    "    df_6 = pd.DataFrame(data_6)\n",
    "\n",
    "    data_7 = {'id': counter_7.counting_list, 'class': 'Blue'}\n",
    "    df_7 = pd.DataFrame(data_7)\n",
    "\n",
    "    df_combined = pd.concat([df_0, df_1, df_2, df_3, df_4, df_5, df_6, df_7], ignore_index=True)\n",
    "    df_combined = df_combined.sort_values(by='id')\n",
    "\n",
    "    excel_filename = 'combined_counts.xlsx'\n",
    "    df_combined.to_excel(excel_filename, index=False)\n",
    "\n",
    "    # Reduce el tamaño de la imagen para su visualización\n",
    "    small_frame = cv2.resize(im0, (int(w * reduction_percentage / 100), int(h * reduction_percentage / 100)))\n",
    "    \n",
    "    # Escribe el frame en el archivo de video\n",
    "    video_writer.write(im0)\n",
    "\n",
    "    # Muestra la imagen reducida\n",
    "    cv2.imshow(\"Object Counting\", small_frame)\n",
    "\n",
    "    # Rompe el bucle si se presiona 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a5e8d61",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mcap\u001b[49m\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m      2\u001b[0m video_writer\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m      3\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cap' is not defined"
     ]
    }
   ],
   "source": [
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f320fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import openpyxl\n",
    "\n",
    "# Datos de counter_dead.counting_list\n",
    "data_dead = {'id': counter_dead.counting_list, 'class': 'dead'}\n",
    "df_dead = pd.DataFrame(data_dead)\n",
    "\n",
    "# Datos de counter_alive.counting_list\n",
    "data_alive = {'id': counter_alive.counting_list, 'class': 'alive'}\n",
    "df_alive = pd.DataFrame(data_alive)\n",
    "\n",
    "# Unir ambos dataframes\n",
    "df_combined = pd.concat([df_dead, df_alive], ignore_index=True)\n",
    "\n",
    "# Ordenar por ID\n",
    "df_combined = df_combined.sort_values(by='id')\n",
    "\n",
    "# Guardar en un archivo Excel\n",
    "excel_filename = 'combined_counts.xlsx'\n",
    "df_combined.to_excel(excel_filename, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc5b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1361f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install shapely\n",
    "!conda activate myenv && pip install shapely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64a64a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shapely\n",
    "\n",
    "print(shapely.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6fcbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate myenv && pip install lapx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d14171f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['T', '__abs__', '__add__', '__and__', '__array__', '__array_finalize__', '__array_function__', '__array_interface__', '__array_prepare__', '__array_priority__', '__array_struct__', '__array_ufunc__', '__array_wrap__', '__bool__', '__class__', '__class_getitem__', '__complex__', '__contains__', '__copy__', '__deepcopy__', '__delattr__', '__delitem__', '__dir__', '__divmod__', '__dlpack__', '__dlpack_device__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__iadd__', '__iand__', '__ifloordiv__', '__ilshift__', '__imatmul__', '__imod__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__mul__', '__ne__', '__neg__', '__new__', '__or__', '__pos__', '__pow__', '__radd__', '__rand__', '__rdivmod__', '__reduce__', '__reduce_ex__', '__repr__', '__rfloordiv__', '__rlshift__', '__rmatmul__', '__rmod__', '__rmul__', '__ror__', '__rpow__', '__rrshift__', '__rshift__', '__rsub__', '__rtruediv__', '__rxor__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__xor__', 'all', 'any', 'argmax', 'argmin', 'argpartition', 'argsort', 'astype', 'base', 'byteswap', 'choose', 'clip', 'compress', 'conj', 'conjugate', 'copy', 'ctypes', 'cumprod', 'cumsum', 'data', 'diagonal', 'dot', 'dtype', 'dump', 'dumps', 'fill', 'flags', 'flat', 'flatten', 'getfield', 'imag', 'item', 'itemset', 'itemsize', 'max', 'mean', 'min', 'nbytes', 'ndim', 'newbyteorder', 'nonzero', 'partition', 'prod', 'ptp', 'put', 'ravel', 'real', 'repeat', 'reshape', 'resize', 'round', 'searchsorted', 'setfield', 'setflags', 'shape', 'size', 'sort', 'squeeze', 'std', 'strides', 'sum', 'swapaxes', 'take', 'tobytes', 'tofile', 'tolist', 'tostring', 'trace', 'transpose', 'var', 'view']\n"
     ]
    }
   ],
   "source": [
    "#tracker1\n",
    "print(dir(counts_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "95a826d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Información de cada elemento de la lista:\n",
      "__class__: <class 'ultralytics.solutions.object_counter.ObjectCounter'>\n",
      "__delattr__: <method-wrapper '__delattr__' of ObjectCounter object at 0x0000022284E42B90>\n",
      "__dict__: {'is_drawing': False, 'selected_point': None, 'reg_pts': [(432.0, 0), (432.0, 1920), (540.0, 1920), (540.0, 0)], 'line_dist_thresh': 15, 'counting_region': <POLYGON ((432 0, 432 1920, 540 1920, 540 0, 432 0))>, 'region_color': (255, 0, 255), 'region_thickness': 5, 'im0': array([[[252, 250, 235],\n",
      "        [252, 250, 235],\n",
      "        [253, 251, 236],\n",
      "        ...,\n",
      "        [206, 178, 159],\n",
      "        [206, 178, 159],\n",
      "        [206, 178, 159]],\n",
      "\n",
      "       [[252, 250, 235],\n",
      "        [252, 250, 235],\n",
      "        [253, 251, 236],\n",
      "        ...,\n",
      "        [204, 176, 157],\n",
      "        [204, 176, 157],\n",
      "        [204, 176, 157]],\n",
      "\n",
      "       [[252, 250, 235],\n",
      "        [252, 250, 235],\n",
      "        [253, 251, 236],\n",
      "        ...,\n",
      "        [205, 177, 158],\n",
      "        [205, 177, 158],\n",
      "        [205, 177, 158]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[ 47,  44,  51],\n",
      "        [ 47,  44,  51],\n",
      "        [ 47,  44,  51],\n",
      "        ...,\n",
      "        [ 66,  61,  69],\n",
      "        [ 66,  61,  69],\n",
      "        [ 66,  61,  69]],\n",
      "\n",
      "       [[ 47,  44,  51],\n",
      "        [ 47,  44,  51],\n",
      "        [ 47,  44,  51],\n",
      "        ...,\n",
      "        [ 66,  61,  69],\n",
      "        [ 66,  61,  69],\n",
      "        [ 66,  61,  69]],\n",
      "\n",
      "       [[ 47,  44,  51],\n",
      "        [ 47,  44,  51],\n",
      "        [ 47,  44,  51],\n",
      "        ...,\n",
      "        [ 66,  61,  69],\n",
      "        [ 66,  61,  69],\n",
      "        [ 66,  61,  69]]], dtype=uint8), 'tf': 2, 'view_img': False, 'view_in_counts': False, 'view_out_counts': False, 'names': {0: 'Dead', 1: 'Alive'}, 'annotator': <ultralytics.utils.plotting.Annotator object at 0x00000222936CB910>, 'in_counts': 0, 'out_counts': 4, 'counting_list': [4, 8, 11, 17], 'count_txt_thickness': 2, 'count_txt_color': (0, 0, 0), 'count_color': (255, 255, 255), 'track_history': defaultdict(<class 'list'>, {4: [(936.9225463867188, 958.21484375), (921.8114013671875, 955.8579711914062), (908.01611328125, 942.8403930664062), (889.32470703125, 959.5477905273438), (887.4158935546875, 955.8292236328125), (885.300048828125, 961.3876953125), (859.684814453125, 958.5404052734375), (860.0623779296875, 967.8726806640625), (844.9864501953125, 988.7839965820312), (819.6116943359375, 984.1397705078125), (736.3682861328125, 998.7425537109375), (475.3539123535156, 1092.408447265625), (444.08831787109375, 1111.28369140625)], 5: [(696.5491943359375, 987.9586181640625), (679.0677490234375, 978.6378784179688), (671.43115234375, 971.95068359375)], 8: [(682.5419921875, 1138.909912109375), (583.9580688476562, 1240.4130859375), (559.3521728515625, 1251.90283203125), (482.4537353515625, 1248.3095703125), (443.8793029785156, 1222.009521484375), (381.312255859375, 1202.188232421875)], 11: [(683.991455078125, 1267.8154296875), (494.6383056640625, 1210.91796875), (474.784912109375, 1258.1226806640625)], 17: [(710.64111328125, 1239.4375), (511.58447265625, 1242.090087890625), (490.63006591796875, 1255.4967041015625)]}), 'track_thickness': 2, 'draw_tracks': True, 'track_color': (0, 255, 0), 'env_check': True, 'counter_variable': 0}\n",
      "__dir__: <built-in method __dir__ of ObjectCounter object at 0x0000022284E42B90>\n",
      "__doc__: A class to manage the counting of objects in a real-time video stream based on their tracks.\n",
      "__eq__: <method-wrapper '__eq__' of ObjectCounter object at 0x0000022284E42B90>\n",
      "__format__: <built-in method __format__ of ObjectCounter object at 0x0000022284E42B90>\n",
      "__ge__: <method-wrapper '__ge__' of ObjectCounter object at 0x0000022284E42B90>\n",
      "__getattribute__: <method-wrapper '__getattribute__' of ObjectCounter object at 0x0000022284E42B90>\n",
      "__getstate__: <built-in method __getstate__ of ObjectCounter object at 0x0000022284E42B90>\n",
      "__gt__: <method-wrapper '__gt__' of ObjectCounter object at 0x0000022284E42B90>\n",
      "__hash__: <method-wrapper '__hash__' of ObjectCounter object at 0x0000022284E42B90>\n",
      "__init__: <bound method ObjectCounter.__init__ of <ultralytics.solutions.object_counter.ObjectCounter object at 0x0000022284E42B90>>\n",
      "__init_subclass__: <built-in method __init_subclass__ of type object at 0x00000222FC1A10E0>\n",
      "__le__: <method-wrapper '__le__' of ObjectCounter object at 0x0000022284E42B90>\n",
      "__lt__: <method-wrapper '__lt__' of ObjectCounter object at 0x0000022284E42B90>\n",
      "__module__: ultralytics.solutions.object_counter\n",
      "__ne__: <method-wrapper '__ne__' of ObjectCounter object at 0x0000022284E42B90>\n",
      "__new__: <built-in method __new__ of type object at 0x00007FFF4B31C330>\n",
      "__reduce__: <built-in method __reduce__ of ObjectCounter object at 0x0000022284E42B90>\n",
      "__reduce_ex__: <built-in method __reduce_ex__ of ObjectCounter object at 0x0000022284E42B90>\n",
      "__repr__: <method-wrapper '__repr__' of ObjectCounter object at 0x0000022284E42B90>\n",
      "__setattr__: <method-wrapper '__setattr__' of ObjectCounter object at 0x0000022284E42B90>\n",
      "__sizeof__: <built-in method __sizeof__ of ObjectCounter object at 0x0000022284E42B90>\n",
      "__str__: <method-wrapper '__str__' of ObjectCounter object at 0x0000022284E42B90>\n",
      "__subclasshook__: <built-in method __subclasshook__ of type object at 0x00000222FC1A10E0>\n",
      "__weakref__: None\n",
      "annotator: <ultralytics.utils.plotting.Annotator object at 0x00000222936CB910>\n",
      "count_color: (255, 255, 255)\n",
      "count_txt_color: (0, 0, 0)\n",
      "count_txt_thickness: 2\n",
      "counter_variable: 0\n",
      "counting_list: [4, 8, 11, 17]\n",
      "counting_region: POLYGON ((432 0, 432 1920, 540 1920, 540 0, 432 0))\n",
      "display_frames: <bound method ObjectCounter.display_frames of <ultralytics.solutions.object_counter.ObjectCounter object at 0x0000022284E42B90>>\n",
      "draw_tracks: True\n",
      "env_check: True\n",
      "extract_and_process_tracks: <bound method ObjectCounter.extract_and_process_tracks of <ultralytics.solutions.object_counter.ObjectCounter object at 0x0000022284E42B90>>\n",
      "im0: [[[252 250 235]\n",
      "  [252 250 235]\n",
      "  [253 251 236]\n",
      "  ...\n",
      "  [206 178 159]\n",
      "  [206 178 159]\n",
      "  [206 178 159]]\n",
      "\n",
      " [[252 250 235]\n",
      "  [252 250 235]\n",
      "  [253 251 236]\n",
      "  ...\n",
      "  [204 176 157]\n",
      "  [204 176 157]\n",
      "  [204 176 157]]\n",
      "\n",
      " [[252 250 235]\n",
      "  [252 250 235]\n",
      "  [253 251 236]\n",
      "  ...\n",
      "  [205 177 158]\n",
      "  [205 177 158]\n",
      "  [205 177 158]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 47  44  51]\n",
      "  [ 47  44  51]\n",
      "  [ 47  44  51]\n",
      "  ...\n",
      "  [ 66  61  69]\n",
      "  [ 66  61  69]\n",
      "  [ 66  61  69]]\n",
      "\n",
      " [[ 47  44  51]\n",
      "  [ 47  44  51]\n",
      "  [ 47  44  51]\n",
      "  ...\n",
      "  [ 66  61  69]\n",
      "  [ 66  61  69]\n",
      "  [ 66  61  69]]\n",
      "\n",
      " [[ 47  44  51]\n",
      "  [ 47  44  51]\n",
      "  [ 47  44  51]\n",
      "  ...\n",
      "  [ 66  61  69]\n",
      "  [ 66  61  69]\n",
      "  [ 66  61  69]]]\n",
      "in_counts: 0\n",
      "is_drawing: False\n",
      "line_dist_thresh: 15\n",
      "mouse_event_for_region: <bound method ObjectCounter.mouse_event_for_region of <ultralytics.solutions.object_counter.ObjectCounter object at 0x0000022284E42B90>>\n",
      "names: {0: 'Dead', 1: 'Alive'}\n",
      "out_counts: 4\n",
      "reg_pts: [(432.0, 0), (432.0, 1920), (540.0, 1920), (540.0, 0)]\n",
      "region_color: (255, 0, 255)\n",
      "region_thickness: 5\n",
      "selected_point: None\n",
      "set_args: <bound method ObjectCounter.set_args of <ultralytics.solutions.object_counter.ObjectCounter object at 0x0000022284E42B90>>\n",
      "start_counting: <bound method ObjectCounter.start_counting of <ultralytics.solutions.object_counter.ObjectCounter object at 0x0000022284E42B90>>\n",
      "tf: 2\n",
      "track_color: (0, 255, 0)\n",
      "track_history: defaultdict(<class 'list'>, {4: [(936.9225463867188, 958.21484375), (921.8114013671875, 955.8579711914062), (908.01611328125, 942.8403930664062), (889.32470703125, 959.5477905273438), (887.4158935546875, 955.8292236328125), (885.300048828125, 961.3876953125), (859.684814453125, 958.5404052734375), (860.0623779296875, 967.8726806640625), (844.9864501953125, 988.7839965820312), (819.6116943359375, 984.1397705078125), (736.3682861328125, 998.7425537109375), (475.3539123535156, 1092.408447265625), (444.08831787109375, 1111.28369140625)], 5: [(696.5491943359375, 987.9586181640625), (679.0677490234375, 978.6378784179688), (671.43115234375, 971.95068359375)], 8: [(682.5419921875, 1138.909912109375), (583.9580688476562, 1240.4130859375), (559.3521728515625, 1251.90283203125), (482.4537353515625, 1248.3095703125), (443.8793029785156, 1222.009521484375), (381.312255859375, 1202.188232421875)], 11: [(683.991455078125, 1267.8154296875), (494.6383056640625, 1210.91796875), (474.784912109375, 1258.1226806640625)], 17: [(710.64111328125, 1239.4375), (511.58447265625, 1242.090087890625), (490.63006591796875, 1255.4967041015625)]})\n",
      "track_thickness: 2\n",
      "view_img: False\n",
      "view_in_counts: False\n",
      "view_out_counts: False\n"
     ]
    }
   ],
   "source": [
    "# Objeto counts_dead\n",
    "print(\"Información de cada elemento de la lista:\")\n",
    "for atributo in dir(counter_all):\n",
    "    # Utiliza getattr() para obtener el valor del atributo\n",
    "    valor_atributo = getattr(counter_all, atributo)\n",
    "    # Imprime el nombre del atributo y su valor\n",
    "    print(f\"{atributo}: {valor_atributo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5161f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acceder a los conteos\n",
    "in_count = counter_dead.in_counts\n",
    "in_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef930d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_count = counter_dead.out_counts\n",
    "out_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "e17c761f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "clases_detectadas = counter_dead.track_history\n",
    "\n",
    "# Obtener solo los ID de cada objeto detectado\n",
    "ids_detectados = list(clases_detectadas.keys())\n",
    "\n",
    "# Imprimir los ID de los objetos detectados\n",
    "print(ids_detectados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "2bc82267",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_dead.counting_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "d938098e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 4, 8, 11, 17]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counter_alive.counting_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40976ddd",
   "metadata": {},
   "source": [
    "## Guardar video en dimensiones personalizadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d4994b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre del archivo de video de entrada\n",
    "video_path = \"8.mp4\"\n",
    "\n",
    "# Nombre del archivo de video de salida (formato MP4)\n",
    "output_path = \"output.mp4\"\n",
    "\n",
    "# Inicializar el lector de video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Obtener las propiedades del video (ancho, alto, fps)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Inicializar el escritor de video\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Resize the frame to a smaller resolution (optional)\n",
    "        frame = cv2.resize(frame, (600, 800))  # Adjust the new_width and new_height\n",
    "\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Write the annotated frame to the output video\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "\n",
    "        # Reduce the delay for higher FPS\n",
    "        # cv2.waitKey(1)  # Set a delay for approximately 60 FPS\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break \n",
    "\n",
    "# Liberar recursos\n",
    "cap.release()\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5b942d",
   "metadata": {},
   "source": [
    "## Guardar video en dimensiones reales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09458580",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el lector de video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Obtener las propiedades del video (ancho, alto, fps)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f76653f",
   "metadata": {},
   "outputs": [],
   "source": [
    "width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa80cc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10389f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar el lector de video\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Obtener las propiedades del video (ancho, alto, fps)\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Inicializar el escritor de video\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Escribir el frame procesado en el video de salida\n",
    "        out.write(annotated_frame)\n",
    "\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break \n",
    "\n",
    "# Liberar recursos\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "print(f\"Video de salida guardado en: {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7374c9c4",
   "metadata": {},
   "source": [
    "## Inferencias en tiempo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973d4aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer la dirección IP y el puerto proporcionados por la aplicación \"IP Webcam\"\n",
    "ip_address = \"http://10.4.41.24:8080\"  # Ruta base proporcionada por la aplicación\n",
    "\n",
    "# URL completa para acceder al video en vivo\n",
    "video_url = f\"{ip_address}/video\"\n",
    "\n",
    "# Obtener la resolución de tu pantalla\n",
    "screen_width = 1920  # Reemplaza con la resolución de tu pantalla\n",
    "screen_height = 1080  # Reemplaza con la resolución de tu pantalla\n",
    "\n",
    "# Crear un objeto VideoCapture para acceder al video de la cámara del teléfono\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "\n",
    "while True:\n",
    "    # Capturar un fotograma de la cámara del teléfono\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Resize the frame to a smaller resolution (optional)\n",
    "        frame = cv2.resize(frame, (screen_width, screen_height))  # Adjust the new_width and new_height\n",
    "\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "\n",
    "        # Reduce the delay for higher FPS\n",
    "        # cv2.waitKey(1)  # Set a delay for approximately 60 FPS\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba6f6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liberar la cámara y cerrar las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08be61c5",
   "metadata": {},
   "source": [
    "## Conteo, Tracking, Registro en tiempo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea95834f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.solutions import object_counter\n",
    "import csv\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import threading\n",
    "\n",
    "model = YOLO(\"G:/Mi unidad/Python/Proyecto-vigor-defoliacion/runs/detect/train8/weights/best.pt/\")\n",
    "\n",
    "excel_filename = 'combined_counts_test.xlsx'\n",
    "\n",
    "# Establecer la dirección IP y el puerto proporcionados por la aplicación \"IP Webcam\"\n",
    "ip_address = \"https://172.20.10.12:8080\"  # Ruta base proporcionada por la aplicación\n",
    "\n",
    "# URL completa para acceder al video en vivo\n",
    "video_url = f\"{ip_address}/video\"\n",
    "#video_url = 0\n",
    "\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1920)\n",
    "cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 1080)\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Porcentaje de reducción de tamaño de la ventana\n",
    "reduction_percentage = 0.4\n",
    "\n",
    "# Define region points in a vertical orientation\n",
    "region_points = [(w*0.40, 0), (w*0.40, h), \n",
    "                 (w*0.50, h), (w*0.50, 0)]  # Ajusta las coordenadas según tus preferencias\n",
    "\n",
    "classes_to_count = [0, 1]\n",
    "\n",
    "# Clases a contar (Dead y Alive)\n",
    "classes_to_count_dead = [0]  # Reemplaza con el índice correcto de la clase \"Dead\"\n",
    "classes_to_count_alive = [1]  # Reemplaza con el índice correcto de la clase \"Alive\"\n",
    "\n",
    "# Video writer\n",
    "video_writer = cv2.VideoWriter(\"object_counting_multiclass_test2.avi\",\n",
    "                       cv2.VideoWriter_fourcc(*'mp4v'),\n",
    "                       fps,\n",
    "                       (w, h))\n",
    "\n",
    "# Init Object Counters for each class\n",
    "counter_dead = object_counter.ObjectCounter()\n",
    "counter_dead.counter_variable = 0\n",
    "counter_dead.set_args(view_img=False,\n",
    "                      view_in_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia adentro\n",
    "                      view_out_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia afuera\n",
    "                      reg_pts=region_points,\n",
    "                      classes_names=model.model.names,\n",
    "                      draw_tracks=True)\n",
    "\n",
    "counter_alive = object_counter.ObjectCounter()\n",
    "counter_alive.counter_variable = 0\n",
    "counter_alive.set_args(view_img=False,\n",
    "                       view_in_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia adentro\n",
    "                       view_out_counts=False,  # Configura esto a False para no mostrar etiquetas de conteo hacia afuera\n",
    "                       reg_pts=region_points,\n",
    "                       classes_names=model.model.names,\n",
    "                       draw_tracks=True)\n",
    "\n",
    "df_combined = pd.DataFrame(columns=['id', 'class'])  # DataFrame compartido entre hilos\n",
    "\n",
    "def procesar_video():\n",
    "    global df_combined\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, im0 = cap.read()\n",
    "        if not success:\n",
    "            print(\"El frame del video está vacío o el procesamiento del video se ha completado correctamente.\")\n",
    "            break\n",
    "\n",
    "        counters_by_class = {\"Dead\": counter_dead, \"Alive\": counter_alive}\n",
    "        \n",
    "        tracks_dead = model.track(im0, persist=True, show=False, classes=classes_to_count_dead)\n",
    "        counts_dead = counters_by_class[\"Dead\"].start_counting(im0, tracks_dead)\n",
    "\n",
    "        tracks_alive = model.track(im0, persist=True, show=False, classes=classes_to_count_alive)\n",
    "        counts_alive = counters_by_class[\"Alive\"].start_counting(im0, tracks_alive)\n",
    "\n",
    "        # Dead Count\n",
    "        text_dead = f'Dead: {counter_dead.out_counts}'\n",
    "        font_dead = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale_dead = 2\n",
    "        font_thickness_dead = 4\n",
    "        font_color_dead = (0, 0, 255)\n",
    "        background_color_dead = (0, 0, 0)\n",
    "\n",
    "        # Get the size of the text\n",
    "        (text_width_dead, text_height_dead), _ = cv2.getTextSize(text_dead, font_dead, font_scale_dead, font_thickness_dead)\n",
    "\n",
    "        # Set the position for the text\n",
    "        text_position_dead = (10, 60)\n",
    "\n",
    "        # Create a black background for the text\n",
    "        background_rect_dead = (text_position_dead[0], text_position_dead[1] - text_height_dead, text_width_dead, text_height_dead)\n",
    "        cv2.rectangle(im0, background_rect_dead, background_color_dead, thickness=cv2.FILLED)\n",
    "\n",
    "        # Put the text on the image\n",
    "        cv2.putText(im0, text_dead, text_position_dead, font_dead, font_scale_dead, font_color_dead, font_thickness_dead, cv2.LINE_AA)\n",
    "\n",
    "        # Alive Count\n",
    "        text_alive = f'Alive: {counter_alive.out_counts}'\n",
    "        font_alive = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale_alive = 2\n",
    "        font_thickness_alive = 4\n",
    "        font_color_alive = (0, 255, 0)\n",
    "        background_color_alive = (0, 0, 0)\n",
    "\n",
    "        # Get the size of the text\n",
    "        (text_width_alive, text_height_alive), _ = cv2.getTextSize(text_alive, font_alive, font_scale_alive, font_thickness_alive)\n",
    "\n",
    "        # Set the position for the text\n",
    "        text_position_alive = (10, 110)\n",
    "\n",
    "        # Create a black background for the text\n",
    "        background_rect_alive = (text_position_alive[0], text_position_alive[1] - text_height_alive, text_width_alive, text_height_alive)\n",
    "        cv2.rectangle(im0, background_rect_alive, background_color_alive, thickness=cv2.FILLED)\n",
    "\n",
    "        # Put the text on the image\n",
    "        cv2.putText(im0, text_alive, text_position_alive, font_alive, font_scale_alive, font_color_alive, font_thickness_alive, cv2.LINE_AA)\n",
    "\n",
    "        data_dead = {'id': counter_dead.counting_list, 'class': 'dead'}\n",
    "        df_dead = pd.DataFrame(data_dead)\n",
    "\n",
    "        data_alive = {'id': counter_alive.counting_list, 'class': 'alive'}\n",
    "        df_alive = pd.DataFrame(data_alive)\n",
    "\n",
    "        df_combined = pd.concat([df_dead, df_alive], ignore_index=True)\n",
    "        df_combined = df_combined.sort_values(by='id')\n",
    "\n",
    "        df_combined.to_excel(excel_filename, index=False)\n",
    "\n",
    "        small_frame = cv2.resize(im0, (int(w * reduction_percentage / 100), int(h * reduction_percentage / 100)))\n",
    "        video_writer.write(im0)\n",
    "\n",
    "        cv2.imshow(\"Object Counting\", small_frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "# Inicia el hilo para procesar el video\n",
    "video_thread = threading.Thread(target=procesar_video)\n",
    "video_thread.start()\n",
    "\n",
    "# Espera a que el hilo de video termine antes de cerrar la aplicación\n",
    "video_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44008b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a3d5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f59f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecer la dirección IP y el puerto proporcionados por la aplicación \"IP Webcam\"\n",
    "ip_address = \"http://10.4.41.24:8080\"  # Ruta base proporcionada por la aplicación\n",
    "\n",
    "# URL completa para acceder al video en vivo\n",
    "video_url = f\"{ip_address}/video\"\n",
    "\n",
    "# Crear un objeto VideoCapture para acceder al video de la cámara del teléfono\n",
    "cap = cv2.VideoCapture(video_url)\n",
    "\n",
    "# Obtener la resolución del fotograma de la cámara\n",
    "frame_width = int(cap.get(3))\n",
    "frame_height = int(cap.get(4))\n",
    "\n",
    "# Inicializar el tiempo de referencia para el control de velocidad\n",
    "start_time = time()\n",
    "\n",
    "# Velocidad objetivo en FPS\n",
    "target_fps = 10  # Puedes ajustar esto a la velocidad deseada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a300eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    # Capturar un fotograma de la cámara del teléfono\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break  # Salir del bucle si no se puede capturar un fotograma\n",
    "\n",
    "    # Run YOLOv8 inference on the frame\n",
    "    results = model(frame)  # Asegúrate de tener tu modelo YOLOv8 definido previamente\n",
    "\n",
    "    # Visualize the results on the frame\n",
    "    annotated_frame = results[0].plot()\n",
    "\n",
    "    # Display the annotated frame\n",
    "    cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "\n",
    "    # Calcular el tiempo que lleva procesar este fotograma\n",
    "    elapsed_time = time() - start_time\n",
    "\n",
    "    # Calcular el retraso necesario para mantener una velocidad específica (por ejemplo, 30 FPS)\n",
    "    delay = max(1, int((1 / target_fps - elapsed_time) * 1000))\n",
    "\n",
    "    # Esperar el tiempo calculado\n",
    "    cv2.waitKey(delay)\n",
    "\n",
    "    # Actualizar el tiempo de referencia para el próximo fotograma\n",
    "    start_time = time()\n",
    "\n",
    "    # Si se presiona la tecla 'q', salir del bucle\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f71cb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liberar la cámara y cerrar las ventanas\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9884a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall opencv-python\n",
    "#!pip uninstall ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2315d9e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3476d348",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ac0389",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f0bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install https://www.msys2.org/\n",
    "# install https://opencv.org/releases/\n",
    "# install https://cmake.org/download/\n",
    "# https://www.gtk.org/docs/installations/windows/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19620ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pacman -S mingw-w64-x86_64-pkg-config\n",
    "#pacman -S mingw-w64-x86_64-gtk2\n",
    "#pacman -S mingw-w64-x86_64-gtk4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c604c451",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install cmake"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
